# Meta Learning


## Model-based (special network architecture)

Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, Timothy P. Lillicrap:
Meta-Learning with Memory-Augmented Neural Networks. ICML 2016: 1842-1850

> They employ a external memory module to storage the reperastion of data from previous tasks. Such a design is similar to knn, except that only the reperastion vectors are stored here, and the memory module is maintained according to the least recently used principle (like cache).

Tsendsuren Munkhdalai, Hong Yu:
Meta Networks. ICML 2017: 2554-2563

> In few-shot learning, it's difficult to parameterize model rapidly through a few examples. They employ loss gradient as meta information and generate fast parameters for learner by a meta-learner. Base learner consists of slow parameters (optimized on specific-task) and fast parameters (generated by meta-leaner with meta-information across tasks).
