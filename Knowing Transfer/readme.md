
# White-Box
   * ## Transfer Learning
      * [Boosting for Transfer Learning.](https://dl.acm.org/doi/abs/10.1145/1273496.1273521) (ICML 2017, 1341 citations)  
   
      * [Robust Learning from Untrusted Sources.](https://arxiv.org/abs/1901.10310) (ICML 2019)  
   
      * [Transfer Learning via Minimizing the PerformanceGap Between Domains](http://papers.nips.cc/paper/9249-transfer-learning-via-minimizing-the-performance-gap-between-domains) (NeurIPS 2019)
   
   * ## Domain Adaptation
      * [Instance Weighting for Domain Adaptation in NLP.](https://www.aclweb.org/anthology/P07-1034.pdf) (ACL 2007, 732 citations)
   
      * [Covariate Shift Adaptation by Importance Weighted Cross Validation.](http://www.jmlr.org/papers/v8/sugiyama07a.html) (JMLR 2007, 550 citations)
   
      * [Learning Bounds for Domain Adaptation.](http://papers.nips.cc/paper/3212-learning-bounds-for-domain-adaptation) (NIPS 2008, 316 citations)
   
      * [Domain Adaptation with Multiple Sources.](http://papers.nips.cc/paper/3550-domain-adaptation-with-multiple-sources) (NIPS 2009, 294 citations)  
   
      * [Algorithms and theory for multiple-source adaptation.](http://papers.nips.cc/paper/8046-algorithms-and-theory-for-multiple-source-adaptation) (NIPS 2018)  
   
   * ## Multi-Task Learning
      * [Adaptive Smoothed Online Multi-Task Learning](http://papers.nips.cc/paper/6433-adaptive-smoothed-online-multi-task-learning) (NIPS 2016)  
   
      * [Multi-task Learning with Labeled and Unlabeled Tasks.](https://dl.acm.org/citation.cfm?id=3305971) (ICML 2017)
   
   
   * ## Few-shot Learning
      * [Meta-Learning with Memory-Augmented Neural Networks.](http://proceedings.mlr.press/v48/santoro16.html) (ICML 2016, 576 citations)  
      > They employ a external memory module (like cache) to storage the reperastion of data from previous tasks.Such a design is similar to knn, except that only the reperastion vectors are stored here.
   
      * [Meta Networks.](https://dl.acm.org/citation.cfm?id=3305945) (ICML 2017, 272 citations)  
      > In few-shot learning, it's difficult to parameterize model rapidly through a few examples. They employ loss gradient as meta information and generate fast parameters for learner by a meta-learner. Base learner consists of slow parameters (optimized on specific-task) and fast parameters (generated by meta-leaner with meta-information across tasks).  
   
      * [Prototypical Networks for Few-shot Learning.](http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning) (NIPS 2017, 1082 citations)  
      > "Prototype", a way imitating humans to understand the world. They build a meta-learner mapping instances to a low-dimensional space, and the prototypes of various categories are obtained by clustering. For target task in few-shot learning, the prototypes are built by meta-learner and answer the queries through neighbor-based method.  
   
# Grey-Box
   * ## Model Transfer
   
   * ## Federated Learning
      * [Communication-Efficient Learning of Deep Networks from Decentralized Data.](http://proceedings.mlr.press/v54/mcmahan17a.html) (AISTATS 2017, 866 citations)
   
      * [Federated Multi-Task Learning](http://papers.nips.cc/paper/7029-federated-multi-task-learning) (NIPS 2017, 240 citations)
   
      * [Agnostic Federated Learning](https://arxiv.org/abs/1902.00146) (ICML 2019, 42 citations)  
      > Learn a robust joint distribution through a min-max game.
   
      * [Bayesian Nonparametric Federated Learning of Neural Networks.](https://arxiv.org/abs/1905.12022) (ICML 2019)
   
      * [Federated Learning with Matched Averaging.](https://arxiv.org/abs/2002.06440) (ICLR 2020)
   
      * [Federated Adversarial Domain Adaptation.](https://arxiv.org/abs/1911.02054) (ICLR 2020)
   
   * ## Distributed Learning
      * [Distributed Robust Learning.](https://arxiv.org/pdf/1409.5937.pdf) (2014)  
      > choose the geometric median of local learned model.  
   
      * [Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates.](https://arxiv.org/abs/1803.01498) (ICML 2018, 99 citations)  
      > Byzantine failure in distributed learning: means that some worker machines may behave completely arbitrarily and can send any message to the master machine. They provided two method: median-based GD and trimmed-mean-based GD, that achieve optimal statistical rates.
      * [Fully Decentralized Joint Learning of Personalized Models and Collaboration Graphs.](https://hal.inria.fr/hal-02166433/) (2019)

# Black-Box
  * [Distilling the Knowledge in a Neural Network.](https://arxiv.org/abs/1503.02531) (2015, 3451 citations)
  
  * [Learning New Tricks From Old Dogs: Multi-Source Transfer Learning From Pre-Trained Networks.](http://papers.nips.cc/paper/8688-learning-new-tricks-from-old-dogs-multi-source-transfer-learning-from-pre-trained-networks) (NeurIPS 2019)
  > This paper has tried to mine the ability of multi-source during transfer. Sepecially, they propose maximal correlation weighting to generate an ensemble module to utilize the internal output from multiple pre-trained networks.
  > In my opinion, it seems like the custom stacking where the second model designed by maximize correlation.  
  > So, what kind of ensemble module is reasonable? robust? efficient? ...
  
  * [Rapid Performance Gain through Active Model Reuse.](http://www.lamda.nju.edu.cn/liyf/paper/ijcai19-acmr.pdf) (IJCAI 2019)



