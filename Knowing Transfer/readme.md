
# White-Box
   ## Transfer Learning
   ## Domain Adaptation
   - [Domain Adaptation with Multiple Sources.] (http://papers.nips.cc/paper/3550-domain-adaptation-with-multiple-sources) (NIPS 2009, 294 citations)
   - [http://papers.nips.cc/paper/8046-algorithms-and-theory-for-multiple-source-adaptation] (http://papers.nips.cc/paper/8046-algorithms-and-theory-for-multiple-source-adaptation) (NIPS 2018)
   ## Few-shot Learning
   - [Meta-Learning with Memory-Augmented Neural Networks.](http://proceedings.mlr.press/v48/santoro16.html) (ICML 2016, 576 citations)
   > They employ a external memory module to storage the reperastion of data from previous tasks. Such a design is similar to knn, except that only the reperastion vectors are stored here, and the memory module is maintained according to the least recently used principle (like cache).
   - [Meta Networks.](https://dl.acm.org/citation.cfm?id=3305945) (ICML 2017, 272 citations)
   > In few-shot learning, it's difficult to parameterize model rapidly through a few examples. They employ loss gradient as meta information and generate fast parameters for learner by a meta-learner. Base learner consists of slow parameters (optimized on specific-task) and fast parameters (generated by meta-leaner with meta-information across tasks).
   [Prototypical Networks for Few-shot Learning.](http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning) (NIPS 2017, 1082 citations)
   > "Prototype", a way imitating humans to understand the world. They build a meta-learner mapping instances to a low-dimensional space, and the prototypes of various categories are obtained by clustering. For target task in few-shot learning, the prototypes are built by meta-learner and answer the queries through neighbor-based method.
# Grey-Box


# Black-Box
  - [Learning New Tricks From Old Dogs: Multi-Source Transfer Learning From Pre-Trained Networks.](http://papers.nips.cc/paper/8688-learning-new-tricks-from-old-dogs-multi-source-transfer-learning-from-pre-trained-networks) (NeurIPS 2019)
  > This paper has tried to mine the ability of multi-source during transfer. Sepecially, they propose maximal correlation weighting to generate an ensemble module to utilize the internal output from multiple pre-trained networks.
  > In my opinion, it seems like the custom stacking where the second model designed by maximize correlation.  
  > So, what kind of ensemble module is reasonable? robust? efficient? ...



